{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ZoeDepth' already exists and is not an empty directory.\n",
      "/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/isl-org/ZoeDepth.git\n",
    "%cd ZoeDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm==0.6.7\n",
      "  Using cached timm-0.6.7-py3-none-any.whl (509 kB)\n",
      "Collecting torch>=1.4 (from timm==0.6.7)\n",
      "  Downloading torch-2.1.1-cp39-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting torchvision (from timm==0.6.7)\n",
      "  Downloading torchvision-0.16.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch>=1.4->timm==0.6.7)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/.conda/lib/python3.9/site-packages (from torch>=1.4->timm==0.6.7) (4.8.0)\n",
      "Collecting sympy (from torch>=1.4->timm==0.6.7)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.4->timm==0.6.7)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.4->timm==0.6.7)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec (from torch>=1.4->timm==0.6.7)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting numpy (from torchvision->timm==0.6.7)\n",
      "  Downloading numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from torchvision->timm==0.6.7)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->timm==0.6.7)\n",
      "  Downloading Pillow-10.1.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.4->timm==0.6.7)\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchvision->timm==0.6.7)\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision->timm==0.6.7)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->torchvision->timm==0.6.7)\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision->timm==0.6.7)\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.4->timm==0.6.7)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp39-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torchvision-0.16.1-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, urllib3, sympy, pillow, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, torchvision, timm\n",
      "Successfully installed MarkupSafe-2.1.3 certifi-2023.7.22 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.10.0 idna-3.4 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.2 pillow-10.1.0 requests-2.31.0 sympy-1.12 timm-0.6.7 torch-2.1.1 torchvision-0.16.1 urllib3-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install timm==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /Users/jerrycheng/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
      "Loaded successfully\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mhelp(\u001b[39m\"\u001b[39m\u001b[39mintel-isl/MiDaS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDPT_BEiT_L_384\u001b[39m\u001b[39m\"\u001b[39m, force_reload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Triggers fresh download of MiDaS repo\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m zoe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mZoeD_N\u001b[39m\u001b[39m\"\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m zoe \u001b[39m=\u001b[39m zoe\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolorize\u001b[39m(value, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray_r\u001b[39m\u001b[39m'\u001b[39m, invalid_val\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m99\u001b[39m, invalid_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, background_color\u001b[39m=\u001b[39m(\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m255\u001b[39m), gamma_corrected\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, value_transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:42\u001b[0m, in \u001b[0;36mDepthModel.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m nn\u001b[39m.\u001b[39mModule:\n\u001b[1;32m     41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.cm\n",
    "import numpy as np\n",
    "\n",
    "from zoedepth.utils.misc import get_image_from_url, colorize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "torch.hub.help(\"intel-isl/MiDaS\", \"DPT_BEiT_L_384\", force_reload=True)  # Triggers fresh download of MiDaS repo\n",
    "zoe = torch.hub.load(\".\", \"ZoeD_N\", source=\"local\", pretrained=True)\n",
    "zoe = zoe.to(\"cuda\")\n",
    "\n",
    "\n",
    "def colorize(value, vmin=None, vmax=None, cmap='gray_r', invalid_val=-99, invalid_mask=None, background_color=(128, 128, 128, 255), gamma_corrected=False, value_transform=None):\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        value = value.detach().cpu().numpy()\n",
    "\n",
    "    value = value.squeeze()\n",
    "    if invalid_mask is None:\n",
    "        invalid_mask = value == invalid_val\n",
    "    mask = np.logical_not(invalid_mask)\n",
    "\n",
    "    # normalize\n",
    "    vmin = np.percentile(value[mask],2) if vmin is None else vmin\n",
    "    vmax = np.percentile(value[mask],85) if vmax is None else vmax\n",
    "    if vmin != vmax:\n",
    "        value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n",
    "    else:\n",
    "        # Avoid 0-division\n",
    "        value = value * 0.\n",
    "\n",
    "    # squeeze last dim if it exists\n",
    "    # grey out the invalid values\n",
    "\n",
    "    value[invalid_mask] = np.nan\n",
    "    cmapper = matplotlib.cm.get_cmap(cmap)\n",
    "    if value_transform:\n",
    "        value = value_transform(value)\n",
    "        # value = value / value.max()\n",
    "    value = cmapper(value, bytes=True)  # (nxmx4)\n",
    "\n",
    "    # img = value[:, :, :]\n",
    "    img = value[...]\n",
    "    img[invalid_mask] = background_color\n",
    "\n",
    "    # gamma correction\n",
    "    img = img / 255\n",
    "    img = np.power(img, 2.2)\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_zoe_depth_map(image):\n",
    "    with torch.autocast(\"cuda\", enabled=True):\n",
    "        depth = model_zoe_n.infer_pil(image)\n",
    "    depth = colorize(depth, cmap=\"gray_r\")\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /Users/jerrycheng/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' # This docstring shows up in hub.help()\\n    MiDaS DPT_BEiT_L_384 model for monocular depth estimation\\n    pretrained (bool): load pretrained weights into model\\n    '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.hub.help(\"intel-isl/MiDaS\", \"DPT_BEiT_L_384\", force_reload=True)  # Triggers fresh download of MiDaS repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/isl-org_ZoeDepth_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
      "Loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/isl-org_ZoeDepth_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config with config_version kitti\n",
      "img_size [384, 768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  768\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_K.pt\n",
      "Loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/isl-org_ZoeDepth_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Downloading: \"https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\" to /Users/jerrycheng/.cache/torch/hub/checkpoints/ZoeD_M12_NK.pt\n",
      "100%|██████████| 1.35G/1.35G [02:12<00:00, 10.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "repo = \"isl-org/ZoeDepth\"\n",
    "# Zoe_N\n",
    "model_zoe_n = torch.hub.load(repo, \"ZoeD_N\", pretrained=True)\n",
    "\n",
    "# Zoe_K\n",
    "model_zoe_k = torch.hub.load(repo, \"ZoeD_K\", pretrained=True)\n",
    "\n",
    "# Zoe_NK\n",
    "model_zoe_nk = torch.hub.load(repo, \"ZoeD_NK\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
      "Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Zoe_N\n",
    "model_zoe_n = torch.hub.load(\".\", \"ZoeD_N\", source=\"local\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
      "Loaded successfully\n",
      "Overwriting config with config_version kitti\n",
      "img_size [384, 768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  768\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_K.pt\n",
      "Loaded successfully\n",
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jerrycheng/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\n",
      "Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from zoedepth.models.builder import build_model\n",
    "from zoedepth.utils.config import get_config\n",
    "\n",
    "# ZoeD_N\n",
    "conf = get_config(\"zoedepth\", \"infer\")\n",
    "model_zoe_n = build_model(conf)\n",
    "\n",
    "# ZoeD_K\n",
    "conf = get_config(\"zoedepth\", \"infer\", config_version=\"kitti\")\n",
    "model_zoe_k = build_model(conf)\n",
    "\n",
    "# ZoeD_NK\n",
    "conf = get_config(\"zoedepth_nk\", \"infer\")\n",
    "model_zoe_nk = build_model(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m resized_image \u001b[39m=\u001b[39m pil_image\u001b[39m.\u001b[39mresize(size_int)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# tuple_array = [[tuple(pixel) for pixel in row] for row in np_array]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# depth_numpy = zoe.infer_pil(pil_image) # as numpy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m depth_pil \u001b[39m=\u001b[39m zoe\u001b[39m.\u001b[39;49minfer_pil(image, output_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpil\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# as 16-bit PIL Image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# depth_tensor = zoe.infer_pil(image, output_type=\"tensor\")  # as torch tensor\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:141\u001b[0m, in \u001b[0;36mDepthModel.infer_pil\u001b[0;34m(self, pil_img, pad_input, with_flip_aug, output_type, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mInference interface for the model for PIL image\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m    output_type (str, optional): output type. Supported values are 'numpy', 'pil' and 'tensor'. Defaults to \"numpy\".\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(pil_img)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 141\u001b[0m out_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(x, pad_input\u001b[39m=\u001b[39;49mpad_input, with_flip_aug\u001b[39m=\u001b[39;49mwith_flip_aug, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m output_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m out_tensor\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:126\u001b[0m, in \u001b[0;36mDepthModel.infer\u001b[0;34m(self, x, pad_input, with_flip_aug, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mInference interface for the model\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    torch.Tensor: output tensor of shape (b, 1, h, w)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m with_flip_aug:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer_with_flip_aug(x, pad_input\u001b[39m=\u001b[39;49mpad_input, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_with_pad_aug(x, pad_input\u001b[39m=\u001b[39mpad_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:110\u001b[0m, in \u001b[0;36mDepthModel.infer_with_flip_aug\u001b[0;34m(self, x, pad_input, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mInference interface for the model with horizontal flip augmentation\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mHorizontal flip augmentation improves the accuracy of the model by averaging the output of the model with and without horizontal flip.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m    torch.Tensor: output tensor of shape (b, 1, h, w)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# infer with horizontal flip and average\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_with_pad_aug(x, pad_input\u001b[39m=\u001b[39;49mpad_input, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    111\u001b[0m out_flip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_with_pad_aug(torch\u001b[39m.\u001b[39mflip(x, dims\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m]), pad_input\u001b[39m=\u001b[39mpad_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m out \u001b[39m=\u001b[39m (out \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mflip(out_flip, dims\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m])) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:88\u001b[0m, in \u001b[0;36mDepthModel._infer_with_pad_aug\u001b[0;34m(self, x, pad_input, fh, fw, upsampling_mode, padding_mode, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         padding \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [pad_h, pad_h]\n\u001b[1;32m     87\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(x, padding, mode\u001b[39m=\u001b[39mpadding_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 88\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer(x)\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]:\n\u001b[1;32m     90\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(out, size\u001b[39m=\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]), mode\u001b[39m=\u001b[39mupsampling_mode, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:55\u001b[0m, in \u001b[0;36mDepthModel._infer\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_infer\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    Inference interface for the model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m        torch.Tensor: output tensor of shape (b, 1, h, w)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(x)[\u001b[39m'\u001b[39m\u001b[39mmetric_depth\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/zoedepth/zoedepth_v1.py:144\u001b[0m, in \u001b[0;36mZoeDepth.forward\u001b[0;34m(self, x, return_final_centers, denorm, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_input_width \u001b[39m=\u001b[39m w\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_input_height \u001b[39m=\u001b[39m h\n\u001b[0;32m--> 144\u001b[0m rel_depth, out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore(x, denorm\u001b[39m=\u001b[39;49mdenorm, return_rel_depth\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    145\u001b[0m \u001b[39m# print(\"output shapes\", rel_depth.shape, out.shape)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m outconv_activation \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/base_models/midas.py:262\u001b[0m, in \u001b[0;36mMidasCore.forward\u001b[0;34m(self, x, denorm, return_rel_depth)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[39mif\u001b[39;00m denorm:\n\u001b[1;32m    261\u001b[0m         x \u001b[39m=\u001b[39m denormalize(x)\n\u001b[0;32m--> 262\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep(x)\n\u001b[1;32m    263\u001b[0m     \u001b[39m# print(\"Shape after prep: \", x.shape)\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable):\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m     \u001b[39m# print(\"Input size to Midascore\", x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/base_models/midas.py:186\u001b[0m, in \u001b[0;36mPrepForMidas.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalization(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresizer(x))\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/base_models/midas.py:173\u001b[0m, in \u001b[0;36mResize.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    172\u001b[0m     width, height \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_size(\u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:][::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mreturn\u001b[39;00m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(x, (height, width), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3924\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m   3923\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(_is_integer(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m size):\n\u001b[0;32m-> 3924\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   3925\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mexpected size to be one of int or Tuple[int] or Tuple[int, int] or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3926\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTuple[int, int, int], but got size with types \u001b[39m\u001b[39m{\u001b[39;00m[\u001b[39mtype\u001b[39m(x)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mx\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39msize]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3927\u001b[0m             )\n\u001b[1;32m   3928\u001b[0m     output_size \u001b[39m=\u001b[39m size\n\u001b[1;32m   3929\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]"
     ]
    }
   ],
   "source": [
    "##### sample prediction\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "zoe = model_zoe_n.to(DEVICE)\n",
    "\n",
    "# Local file\n",
    "from PIL import Image\n",
    "image = Image.open(\"/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/data_set/my_dataset/photos/stair_4.jpg\").convert(\"RGB\")  # load\n",
    "np_array = np.array(image)\n",
    "pil_image = Image.fromarray(np_array)\n",
    "\n",
    "# Assuming size is a tuple with numpy.int64 elements\n",
    "width, height = 200, 300  # size derived from a NumPy array\n",
    "size_int = (int(width), int(height))\n",
    "\n",
    "# Use size_int for operations that expect size parameters\n",
    "resized_image = pil_image.resize(size_int)\n",
    "\n",
    "\n",
    "# tuple_array = [[tuple(pixel) for pixel in row] for row in np_array]\n",
    "# depth_numpy = zoe.infer_pil(pil_image) # as numpy\n",
    "\n",
    "depth_pil = zoe.infer_pil(image, output_type=\"pil\")  # as 16-bit PIL Image\n",
    "\n",
    "# depth_tensor = zoe.infer_pil(image, output_type=\"tensor\")  # as torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 60, 52)\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(image)\n",
    "tuple_array = [[tuple(pixel) for pixel in row] for row in np_array]\n",
    "print(tuple_array[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m URL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4W8H_Nxk_rs3Vje_zj6mglPOH7bnPhQitBH8WkqjlqQVotdtDEG37BsnGofME3_u6lDk&usqp=CAU\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m image \u001b[39m=\u001b[39m get_image_from_url(URL)  \u001b[39m# fetch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m depth \u001b[39m=\u001b[39m zoe\u001b[39m.\u001b[39;49minfer_pil(image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Save raw\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrycheng/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoe_depth.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mzoedepth\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m save_raw_16bit\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:141\u001b[0m, in \u001b[0;36mDepthModel.infer_pil\u001b[0;34m(self, pil_img, pad_input, with_flip_aug, output_type, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mInference interface for the model for PIL image\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m    output_type (str, optional): output type. Supported values are 'numpy', 'pil' and 'tensor'. Defaults to \"numpy\".\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(pil_img)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 141\u001b[0m out_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(x, pad_input\u001b[39m=\u001b[39;49mpad_input, with_flip_aug\u001b[39m=\u001b[39;49mwith_flip_aug, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m output_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m out_tensor\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:126\u001b[0m, in \u001b[0;36mDepthModel.infer\u001b[0;34m(self, x, pad_input, with_flip_aug, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mInference interface for the model\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    torch.Tensor: output tensor of shape (b, 1, h, w)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m with_flip_aug:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer_with_flip_aug(x, pad_input\u001b[39m=\u001b[39;49mpad_input, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_with_pad_aug(x, pad_input\u001b[39m=\u001b[39mpad_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:110\u001b[0m, in \u001b[0;36mDepthModel.infer_with_flip_aug\u001b[0;34m(self, x, pad_input, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mInference interface for the model with horizontal flip augmentation\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mHorizontal flip augmentation improves the accuracy of the model by averaging the output of the model with and without horizontal flip.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m    torch.Tensor: output tensor of shape (b, 1, h, w)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# infer with horizontal flip and average\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_with_pad_aug(x, pad_input\u001b[39m=\u001b[39;49mpad_input, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    111\u001b[0m out_flip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_with_pad_aug(torch\u001b[39m.\u001b[39mflip(x, dims\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m]), pad_input\u001b[39m=\u001b[39mpad_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m out \u001b[39m=\u001b[39m (out \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mflip(out_flip, dims\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m])) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:88\u001b[0m, in \u001b[0;36mDepthModel._infer_with_pad_aug\u001b[0;34m(self, x, pad_input, fh, fw, upsampling_mode, padding_mode, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         padding \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [pad_h, pad_h]\n\u001b[1;32m     87\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(x, padding, mode\u001b[39m=\u001b[39mpadding_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 88\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer(x)\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]:\n\u001b[1;32m     90\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(out, size\u001b[39m=\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]), mode\u001b[39m=\u001b[39mupsampling_mode, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/depth_model.py:55\u001b[0m, in \u001b[0;36mDepthModel._infer\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_infer\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    Inference interface for the model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m        torch.Tensor: output tensor of shape (b, 1, h, w)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(x)[\u001b[39m'\u001b[39m\u001b[39mmetric_depth\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/zoedepth/zoedepth_v1.py:144\u001b[0m, in \u001b[0;36mZoeDepth.forward\u001b[0;34m(self, x, return_final_centers, denorm, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_input_width \u001b[39m=\u001b[39m w\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_input_height \u001b[39m=\u001b[39m h\n\u001b[0;32m--> 144\u001b[0m rel_depth, out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore(x, denorm\u001b[39m=\u001b[39;49mdenorm, return_rel_depth\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    145\u001b[0m \u001b[39m# print(\"output shapes\", rel_depth.shape, out.shape)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m outconv_activation \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/base_models/midas.py:262\u001b[0m, in \u001b[0;36mMidasCore.forward\u001b[0;34m(self, x, denorm, return_rel_depth)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[39mif\u001b[39;00m denorm:\n\u001b[1;32m    261\u001b[0m         x \u001b[39m=\u001b[39m denormalize(x)\n\u001b[0;32m--> 262\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep(x)\n\u001b[1;32m    263\u001b[0m     \u001b[39m# print(\"Shape after prep: \", x.shape)\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable):\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m     \u001b[39m# print(\"Input size to Midascore\", x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/base_models/midas.py:186\u001b[0m, in \u001b[0;36mPrepForMidas.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalization(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresizer(x))\n",
      "File \u001b[0;32m~/Desktop/AER1515/Project/depth_estimation_git/depth_estimation/ZoeDepth/zoedepth/models/base_models/midas.py:173\u001b[0m, in \u001b[0;36mResize.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    172\u001b[0m     width, height \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_size(\u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:][::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mreturn\u001b[39;00m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(x, (height, width), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3924\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m   3923\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(_is_integer(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m size):\n\u001b[0;32m-> 3924\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   3925\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mexpected size to be one of int or Tuple[int] or Tuple[int, int] or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3926\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTuple[int, int, int], but got size with types \u001b[39m\u001b[39m{\u001b[39;00m[\u001b[39mtype\u001b[39m(x)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mx\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39msize]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3927\u001b[0m             )\n\u001b[1;32m   3928\u001b[0m     output_size \u001b[39m=\u001b[39m size\n\u001b[1;32m   3929\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]"
     ]
    }
   ],
   "source": [
    "##### sample prediction\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "zoe = model_zoe_n.to(DEVICE)\n",
    "\n",
    "\n",
    "# From URL\n",
    "from zoedepth.utils.misc import get_image_from_url\n",
    "\n",
    "# Example URL\n",
    "URL = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4W8H_Nxk_rs3Vje_zj6mglPOH7bnPhQitBH8WkqjlqQVotdtDEG37BsnGofME3_u6lDk&usqp=CAU\"\n",
    "\n",
    "\n",
    "image = get_image_from_url(URL)  # fetch\n",
    "depth = zoe.infer_pil(image)\n",
    "\n",
    "# Save raw\n",
    "from zoedepth.utils.misc import save_raw_16bit\n",
    "fpath = \"/path/to/output.png\"\n",
    "save_raw_16bit(depth, fpath)\n",
    "\n",
    "# Colorize output\n",
    "from zoedepth.utils.misc import colorize\n",
    "\n",
    "colored = colorize(depth)\n",
    "\n",
    "# save colored output\n",
    "fpath_colored = \"/path/to/output_colored.png\"\n",
    "Image.fromarray(colored).save(fpath_colored)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

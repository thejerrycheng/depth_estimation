{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - Single image depth estimation -- this could be achieved via CNN and traditional method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define directories\n",
    "RGB_DIR = './data/rgb/'\n",
    "DEPTH_DIR = './data/depth/'\n",
    "\n",
    "# Load dataset filenames\n",
    "rgb_files = sorted([os.path.join(RGB_DIR, f) for f in os.listdir(RGB_DIR) if f.endswith('.png')])\n",
    "depth_files = sorted([os.path.join(DEPTH_DIR, f) for f in os.listdir(DEPTH_DIR) if f.endswith('.png')])\n",
    "\n",
    "# Assuming that the images are matched, i.e., rgb_files[i] corresponds to depth_files[i]\n",
    "assert len(rgb_files) == len(depth_files), \"Mismatch in dataset sizes\"\n",
    "\n",
    "# Load images into memory (you may want to resize or preprocess them further)\n",
    "rgb_images = [cv2.imread(f) for f in rgb_files]\n",
    "depth_images = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in depth_files]  # Assuming depth images are grayscale\n",
    "\n",
    "# Convert to numpy arrays and normalize\n",
    "rgb_images = np.array(rgb_images) / 255.0\n",
    "depth_images = np.array(depth_images) / 255.0\n",
    "\n",
    "# Split dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(rgb_images, depth_images, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Test set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 48, 48, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 24, 24, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 12, 12, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 24, 24, 128)       295040    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 48, 48, 64)        73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 48, 48, 1)         577       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 740225 (2.82 MB)\n",
      "Trainable params: 740225 (2.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def depth_estimation_model(input_shape=(96, 96, 3)):\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Output depth map, sigmoid activation to ensure output is between 0 and 1 (black and white)\n",
    "    outputs = layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = depth_estimation_model()\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the following data loaded and preprocessed:\n",
    "# train_images, train_depths, val_images, val_depths, test_images, test_depths\n",
    "\n",
    "# Training\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "history = model.fit(train_images, train_depths, validation_data=(val_images, val_depths), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluation\n",
    "val_loss = model.evaluate(val_images, val_depths, verbose=1)\n",
    "print(f\"Validation MSE Loss: {val_loss}\")\n",
    "\n",
    "# Testing\n",
    "predicted_depths = model.predict(test_images)\n",
    "\n",
    "# Visualization (Optional: To visualize a test image and its predicted depth map)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 0  # Change this to view different test samples\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_images[idx])\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_depths[idx].reshape(96,96), cmap='gray')\n",
    "plt.title(\"Predicted Depth\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
